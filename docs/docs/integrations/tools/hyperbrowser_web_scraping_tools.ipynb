{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "sidebar_label: Hyperbrowser Web Scraping Tools\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperbrowser Web Scraping Tools\n",
    "\n",
    "[Hyperbrowser](https://hyperbrowser.ai) is a platform for running and scaling headless browsers. It lets you launch and manage browser sessions at scale and provides easy to use solutions for any webscraping needs, such as scraping a single page or crawling an entire site.\n",
    "\n",
    "Key Features:\n",
    "- Instant Scalability - Spin up hundreds of browser sessions in seconds without infrastructure headaches\n",
    "- Simple Integration - Works seamlessly with popular tools like Puppeteer and Playwright\n",
    "- Powerful APIs - Easy to use APIs for scraping/crawling any site, and much more\n",
    "- Bypass Anti-Bot Measures - Built-in stealth mode, ad blocking, automatic CAPTCHA solving, and rotating proxies\n",
    "\n",
    "This notebook provides a quick overview for getting started with Hyperbrowser web tools.\n",
    "\n",
    "For more information about Hyperbrowser, please visit the [Hyperbrowser website](https://hyperbrowser.ai) or if you want to check out the docs, you can visit the [Hyperbrowser docs](https://docs.hyperbrowser.ai).\n",
    "\n",
    "## Key Capabilities\n",
    "\n",
    "### Scrape\n",
    "\n",
    "Hyperbrowser provides powerful scraping capabilities that allow you to extract data from any webpage. The scraping tool can convert web content into structured formats like markdown or HTML, making it easy to process and analyze the data.\n",
    "\n",
    "### Crawl\n",
    "\n",
    "The crawling functionality enables you to navigate through multiple pages of a website automatically. You can set parameters like page limits and depth to control how extensively the crawler explores the site, collecting data from each page it visits.\n",
    "\n",
    "### Extract\n",
    "\n",
    "Hyperbrowser's extraction capabilities use AI to pull specific information from webpages according to your defined schema. This allows you to transform unstructured web content into structured data that matches your exact requirements.\n",
    "\n",
    "\n",
    "\n",
    "## Overview\n",
    "\n",
    "### Integration details\n",
    "\n",
    "| Tool         | Package                | Local | Serializable | [JS support](https://js.langchain.com/docs/integrations/document_loaders/web_loaders/langchain_hyperbrowser_loader) |\n",
    "| :----------- | :--------------------- | :---: | :----------: | :-----------------------------------------------------------------------------------------------------------------: |\n",
    "| Crawl Tool   | langchain-hyperbrowser |  ❌   |      ❌      |                                                         ❌                                                          |\n",
    "| Scrape Tool  | langchain-hyperbrowser |  ❌   |      ❌      |                                                         ❌                                                          |\n",
    "| Extract Tool | langchain-hyperbrowser |  ❌   |      ❌      |                                                         ❌                                                          |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "To access the Hyperbrowser web tools you'll need to install the `langchain-hyperbrowser` integration package, and create a Hyperbrowser account and get an API key.\n",
    "\n",
    "### Credentials\n",
    "\n",
    "Head to [Hyperbrowser](https://app.hyperbrowser.ai/) to sign up and generate an API key. Once you've done this set the HYPERBROWSER_API_KEY environment variable:\n",
    "\n",
    "```bash\n",
    "export HYPERBROWSER_API_KEY=<your-api-key>\n",
    "```\n",
    "\n",
    "### Installation\n",
    "\n",
    "Install **langchain-hyperbrowser**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU langchain-hyperbrowser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiation\n",
    "\n",
    "### Crawl Tool\n",
    "\n",
    "The `HyperbrowserCrawlTool` is a powerful tool that can crawl entire websites, starting from a given URL. It supports configurable page limits and scraping options.\n",
    "\n",
    "```python\n",
    "from langchain_hyperbrowser import HyperbrowserCrawlTool\n",
    "tool = HyperbrowserCrawlTool()\n",
    "```\n",
    "\n",
    "### Scrape Tool\n",
    "\n",
    "The `HyperbrowserScrapeTool` is a tool that can scrape content from web pages. It supports both markdown and HTML output formats, along with metadata extraction.\n",
    "\n",
    "```python\n",
    "from langchain_hyperbrowser import HyperbrowserScrapeTool\n",
    "tool = HyperbrowserScrapeTool()\n",
    "```\n",
    "\n",
    "### Extract Tool\n",
    "\n",
    "The `HyperbrowserExtractTool` is a powerful tool that uses AI to extract structured data from web pages. It can extract information based predefined schemas.\n",
    "\n",
    "```python\n",
    "from langchain_hyperbrowser import HyperbrowserExtractTool\n",
    "tool = HyperbrowserExtractTool()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invocation\n",
    "\n",
    "### Basic Usage\n",
    "\n",
    "#### Crawl Tool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': [CrawledPage(metadata={'url': 'https://www.example.com/', 'title': 'Example Domain', 'viewport': 'width=device-width, initial-scale=1', 'sourceURL': 'https://example.com'}, html=None, markdown='Example Domain\\n\\n# Example Domain\\n\\nThis domain is for use in illustrative examples in documents. You may use this\\ndomain in literature without prior coordination or asking for permission.\\n\\n[More information...](https://www.iana.org/domains/example)', links=None, screenshot=None, url='https://example.com', status='completed', error=None)], 'error': None}\n"
     ]
    }
   ],
   "source": [
    "from langchain_hyperbrowser import HyperbrowserCrawlTool\n",
    "\n",
    "result = HyperbrowserCrawlTool().invoke(\n",
    "    {\n",
    "        \"url\": \"https://example.com\",\n",
    "        \"max_pages\": 2,\n",
    "        \"scrape_options\": {\"formats\": [\"markdown\"]},\n",
    "    }\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrape Tool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': ScrapeJobData(metadata={'url': 'https://www.example.com/', 'title': 'Example Domain', 'viewport': 'width=device-width, initial-scale=1', 'sourceURL': 'https://example.com'}, html=None, markdown='Example Domain\\n\\n# Example Domain\\n\\nThis domain is for use in illustrative examples in documents. You may use this\\ndomain in literature without prior coordination or asking for permission.\\n\\n[More information...](https://www.iana.org/domains/example)', links=None, screenshot=None), 'error': None}\n"
     ]
    }
   ],
   "source": [
    "from langchain_hyperbrowser import HyperbrowserScrapeTool\n",
    "\n",
    "result = HyperbrowserScrapeTool().invoke(\n",
    "    {\"url\": \"https://example.com\", \"scrape_options\": {\"formats\": [\"markdown\"]}}\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract Tool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': {'title': 'Example Domain'}, 'error': None}\n"
     ]
    }
   ],
   "source": [
    "from langchain_hyperbrowser import HyperbrowserExtractTool\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class SimpleExtractionModel(BaseModel):\n",
    "    title: str\n",
    "\n",
    "\n",
    "result = HyperbrowserExtractTool().invoke(\n",
    "    {\n",
    "        \"url\": \"https://example.com\",\n",
    "        \"schema\": SimpleExtractionModel,\n",
    "    }\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Custom Options\n",
    "\n",
    "#### Crawl Tool with Custom Options\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': [CrawledPage(metadata={'url': 'https://www.example.com/', 'title': 'Example Domain', 'viewport': 'width=device-width, initial-scale=1', 'sourceURL': 'https://example.com'}, html=None, markdown='Example Domain\\n\\n# Example Domain\\n\\nThis domain is for use in illustrative examples in documents. You may use this\\ndomain in literature without prior coordination or asking for permission.\\n\\n[More information...](https://www.iana.org/domains/example)', links=None, screenshot=None, url='https://example.com', status='completed', error=None)], 'error': None}\n"
     ]
    }
   ],
   "source": [
    "result = HyperbrowserCrawlTool().run(\n",
    "    {\n",
    "        \"url\": \"https://example.com\",\n",
    "        \"max_pages\": 2,\n",
    "        \"scrape_options\": {\n",
    "            \"formats\": [\"markdown\", \"html\"],\n",
    "        },\n",
    "        \"session_options\": {\"use_proxy\": True, \"solve_captchas\": True},\n",
    "    }\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrape Tool with Custom Options\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': ScrapeJobData(metadata={'url': 'https://www.example.com/', 'title': 'Example Domain', 'viewport': 'width=device-width, initial-scale=1', 'sourceURL': 'https://example.com'}, html='<html><head>\\n    <title>Example Domain</title>\\n\\n    <meta charset=\"utf-8\">\\n    <meta http-equiv=\"Content-type\" content=\"text/html; charset=utf-8\">\\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\\n        \\n</head>\\n\\n<body>\\n<div>\\n    <h1>Example Domain</h1>\\n    <p>This domain is for use in illustrative examples in documents. You may use this\\n    domain in literature without prior coordination or asking for permission.</p>\\n    <p><a href=\"https://www.iana.org/domains/example\">More information...</a></p>\\n</div>\\n\\n\\n</body></html>', markdown='Example Domain\\n\\n# Example Domain\\n\\nThis domain is for use in illustrative examples in documents. You may use this\\ndomain in literature without prior coordination or asking for permission.\\n\\n[More information...](https://www.iana.org/domains/example)', links=None, screenshot=None), 'error': None}\n"
     ]
    }
   ],
   "source": [
    "result = HyperbrowserScrapeTool().run(\n",
    "    {\n",
    "        \"url\": \"https://example.com\",\n",
    "        \"scrape_options\": {\n",
    "            \"formats\": [\"markdown\", \"html\"],\n",
    "        },\n",
    "        \"session_options\": {\"use_proxy\": True, \"solve_captchas\": True},\n",
    "    }\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract Tool with Custom Schema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': {'products': [{'price': 9.99, 'title': 'Essence Mascara Lash Princess'}, {'price': 19.99, 'title': 'Eyeshadow Palette with Mirror'}, {'price': 14.99, 'title': 'Powder Canister'}, {'price': 12.99, 'title': 'Red Lipstick'}, {'price': 8.99, 'title': 'Red Nail Polish'}, {'price': 49.99, 'title': 'Calvin Klein CK One'}, {'price': 129.99, 'title': 'Chanel Coco Noir Eau De'}, {'price': 89.99, 'title': \"Dior J'adore\"}, {'price': 69.99, 'title': 'Dolce Shine Eau de'}, {'price': 79.99, 'title': 'Gucci Bloom Eau de'}]}, 'error': None}\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class ProductSchema(BaseModel):\n",
    "    title: str\n",
    "    price: float\n",
    "\n",
    "\n",
    "class ProductsSchema(BaseModel):\n",
    "    products: List[ProductSchema]\n",
    "\n",
    "\n",
    "result = HyperbrowserExtractTool().run(\n",
    "    {\n",
    "        \"url\": \"https://dummyjson.com/products?limit=10\",\n",
    "        \"schema\": ProductsSchema,\n",
    "        \"session_options\": {\"session_options\": {\"use_proxy\": True}},\n",
    "    }\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Async Usage\n",
    "\n",
    "All tools support async usage:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def web_operations():\n",
    "    # Crawl\n",
    "    crawl_tool = HyperbrowserCrawlTool()\n",
    "    crawl_result = await crawl_tool.arun(\n",
    "        {\n",
    "            \"url\": \"https://example.com\",\n",
    "            \"max_pages\": 5,\n",
    "            \"scrape_options\": {\"formats\": [\"markdown\"]},\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Scrape\n",
    "    scrape_tool = HyperbrowserScrapeTool()\n",
    "    scrape_result = await scrape_tool.arun(\n",
    "        {\"url\": \"https://example.com\", \"scrape_options\": {\"formats\": [\"markdown\"]}}\n",
    "    )\n",
    "\n",
    "    # Extract\n",
    "    extract_tool = HyperbrowserExtractTool()\n",
    "    extract_result = await extract_tool.arun(\n",
    "        {\n",
    "            \"url\": \"https://example.com\",\n",
    "            \"extraction_prompt\": \"Extract the main content\",\n",
    "            \"json_schema\": None,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return crawl_result, scrape_result, extract_result\n",
    "\n",
    "\n",
    "results = await web_operations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use within an agent\n",
    "\n",
    "Here's how to use any of the web tools within an agent:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `hyperbrowser_crawl_data` with `{'url': 'https://example.com', 'max_pages': 5, 'scrape_options': {'formats': ['markdown']}}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m{'data': [CrawledPage(metadata={'url': 'https://www.example.com/', 'title': 'Example Domain', 'viewport': 'width=device-width, initial-scale=1', 'sourceURL': 'https://example.com'}, html=None, markdown='Example Domain\\n\\n# Example Domain\\n\\nThis domain is for use in illustrative examples in documents. You may use this\\ndomain in literature without prior coordination or asking for permission.\\n\\n[More information...](https://www.iana.org/domains/example)', links=None, screenshot=None, url='https://example.com', status='completed', error=None)], 'error': None}\u001b[0m\u001b[32;1m\u001b[1;3mI have crawled the website [Example Domain](https://www.example.com/) and extracted content from the homepage. Here is the content:\n",
      "\n",
      "---\n",
      "\n",
      "# Example Domain\n",
      "\n",
      "This domain is for use in illustrative examples in documents. You may use this domain in literature without prior coordination or asking for permission.\n",
      "\n",
      "[More information...](https://www.iana.org/domains/example)\n",
      "\n",
      "---\n",
      "\n",
      "If you need content from more pages or specific information, feel free to let me know!\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': 'Crawl https://example.com and get content from up to 5 pages', 'output': 'I have crawled the website [Example Domain](https://www.example.com/) and extracted content from the homepage. Here is the content:\\n\\n---\\n\\n# Example Domain\\n\\nThis domain is for use in illustrative examples in documents. You may use this domain in literature without prior coordination or asking for permission.\\n\\n[More information...](https://www.iana.org/domains/example)\\n\\n---\\n\\nIf you need content from more pages or specific information, feel free to let me know!'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import AgentExecutor, create_openai_functions_agent\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_hyperbrowser import HyperbrowserCrawlTool\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Initialize the crawl tool\n",
    "crawl_tool = HyperbrowserCrawlTool()\n",
    "\n",
    "# Create the agent with the crawl tool\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant that crawls websites and extracts content.\",\n",
    "        ),\n",
    "        (\"human\", \"{input}\"),\n",
    "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "    ]\n",
    ")\n",
    "agent = create_openai_functions_agent(llm, [crawl_tool], prompt=prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=[crawl_tool], verbose=True)\n",
    "\n",
    "# Run the agent\n",
    "result = agent_executor.invoke(\n",
    "    {\"input\": \"Crawl https://example.com and get content from up to 5 pages\"}\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration Options\n",
    "\n",
    "### Common Options\n",
    "\n",
    "All tools support these basic configuration options:\n",
    "\n",
    "- `url`: The URL to process\n",
    "- `session_options`: Browser session configuration\n",
    "  - `use_proxy`: Whether to use a proxy\n",
    "  - `solve_captchas`: Whether to automatically solve CAPTCHAs\n",
    "  - `accept_cookies`: Whether to accept cookies\n",
    "\n",
    "### Tool-Specific Options\n",
    "\n",
    "#### Crawl Tool\n",
    "- `max_pages`: Maximum number of pages to crawl\n",
    "- `scrape_options`: Options for scraping each page\n",
    "  - `formats`: List of output formats (markdown, html)\n",
    "\n",
    "#### Scrape Tool\n",
    "- `scrape_options`: Options for scraping the page\n",
    "  - `formats`: List of output formats (markdown, html)\n",
    "\n",
    "#### Extract Tool\n",
    "- `schema`: Pydantic model defining the structure to extract\n",
    "- `extraction_prompt`: Natural language prompt for extraction\n",
    "\n",
    "For more details, see the respective API references:\n",
    "- [Crawl API Reference](https://docs.hyperbrowser.ai/reference/api-reference/crawl)\n",
    "- [Scrape API Reference](https://docs.hyperbrowser.ai/reference/api-reference/scrape)\n",
    "- [Extract API Reference](https://docs.hyperbrowser.ai/reference/api-reference/extract)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API reference\n",
    "\n",
    "- [GitHub](https://github.com/hyperbrowserai/langchain-hyperbrowser/)\n",
    "- [PyPi](https://pypi.org/project/langchain-hyperbrowser/)\n",
    "- [Hyperbrowser Docs](https://docs.hyperbrowser.ai/)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
